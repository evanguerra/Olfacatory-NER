{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Adapted code from https://github.com/Odeuropa/wp3-information-extraction-system"
      ],
      "metadata": {
        "id": "EFQh3ZBve7qm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujubLVmra5ld",
        "outputId": "7172a0b2-0c5f-439a-b0b8-ca310ab6651d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XseCclgred0Z",
        "outputId": "3ef04c09-1772-4046-8db3-470e077daf58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Text Mining/Project\n"
          ]
        }
      ],
      "source": [
        "cd 'drive/MyDrive/Text Mining/Project'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtTR61MDesQp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WhlcJRLgTHa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "050d5f0f-3a3d-4c22-89c6-bc273fb27cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.17.1 dill-0.3.8 multiprocess-0.70.16\n",
            "Collecting accelerate==0.20.3\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate==0.20.3) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate==0.20.3) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.20.3\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install accelerate==0.20.3\n",
        "!pip install tokenizers -q\n",
        "!pip install seqeval -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BL-OQGe29gIB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e999f676-ccf0-446e-e5bd-6aa8fffa381d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.27)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.9.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.2 alembic-1.13.1 colorlog-6.8.2 optuna-3.5.0\n",
            "Collecting ray[tune]\n",
            "  Downloading ray-2.9.3-cp310-cp310-manylinux2014_x86_64.whl (64.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (23.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.5.3)\n",
            "Collecting tensorboardX>=1.9 (from ray[tune])\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (14.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2023.6.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow>=6.0.1->ray[tune]) (1.25.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->ray[tune]) (1.16.0)\n",
            "Installing collected packages: tensorboardX, ray\n",
            "Successfully installed ray-2.9.3 tensorboardX-2.6.2.2\n",
            "Collecting sigopt\n",
            "  Downloading sigopt-8.8.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff<2.0.0,>=1.10.0 (from sigopt)\n",
            "  Downloading backoff-1.11.1-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from sigopt) (8.1.7)\n",
            "Collecting GitPython>=2.0.0 (from sigopt)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from sigopt) (23.2)\n",
            "Collecting pypng>=0.0.20 (from sigopt)\n",
            "  Downloading pypng-0.20220715.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<7,>=5 in /usr/local/lib/python3.10/dist-packages (from sigopt) (6.0.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from sigopt) (2.31.0)\n",
            "Collecting urllib3<2.0.0,>=1.26.5 (from sigopt)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython>=2.0.0->sigopt)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->sigopt) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->sigopt) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->sigopt) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=2.0.0->sigopt)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pypng, urllib3, smmap, backoff, gitdb, GitPython, sigopt\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "Successfully installed GitPython-3.1.42 backoff-1.11.1 gitdb-4.0.11 pypng-0.20220715.0 sigopt-8.8.2 smmap-5.0.1 urllib3-1.26.18\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.3-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.42)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.40.6-py2.py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.5/258.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Installing collected packages: setproctitle, sentry-sdk, docker-pycreds, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 sentry-sdk-1.40.6 setproctitle-1.3.3 wandb-0.16.3\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install 'ray[tune]'\n",
        "!pip install sigopt\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_xdoD_bTqPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb95d549-cb31-48ca-bacb-e200513b9b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.11 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.11->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.11->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.20.3\n",
            "    Uninstalling accelerate-0.20.3:\n",
            "      Successfully uninstalled accelerate-0.20.3\n",
            "Successfully installed accelerate-0.27.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GBPSG7b4VHQ",
        "outputId": "138bcc4c-24cb-4b0f-9b8e-a4d992bfa6e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from torch import cuda\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForTokenClassification, AutoConfig\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "from datasets import load_metric, Dataset\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import argparse\n",
        "import csv\n",
        "import sys\n",
        "from os import path\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "seed = 22\n",
        "transformers.set_seed(seed)\n",
        "\n",
        "def sentence_num(row):\n",
        "    sentenceNum = row['Sentence-Token'].split(\"-\")[0]\n",
        "    return sentenceNum\n",
        "\n",
        "\n",
        "def to_label_id(row, id_dict):\n",
        "    label = row['Tag']\n",
        "    if label not in id_dict:\n",
        "        label = 'O'\n",
        "\n",
        "    labelId = id_dict[label]\n",
        "    return labelId\n",
        "\n",
        "def to_clean_label(row):\n",
        "    clean_tag = row['Tag'].replace(\"\\\\\", \"\").replace(\"\\_\",\"_\")\n",
        "    clean_tag = clean_tag.split('|')[0]\n",
        "    clean_tag = clean_tag.replace(\"B-I-\", \"B-\")\n",
        "    return clean_tag\n",
        "\n",
        "def replace_punctuation(row):\n",
        "    \"\"\"Error case in Italian: 'bianco', '-', 'gialliccio' -> 'bianco-gialliccio'\n",
        "    Bert tokenizer uses also punctuations to separate the tokens along with the whitespaces, although we provide the\n",
        "    sentences with is_split_into_words=True. Therefore, if there is a punctuation in a single word in a CONLL file\n",
        "    we cannot 100% guarantee the exact same surface realization (necessary to decide on a single label for a single word)\n",
        "    after classification for that specific word:\n",
        "    e.g., bianco-gialliccio becomes 3 separate CONLL lines: 1) bianco 2) - 3) gialliccio\n",
        "    Things could have been easier and faster if we were delivering simple sentences as output instead of the exact\n",
        "    CONLL file structure given as input. \"\"\"\n",
        "    word = row['Word'].strip()\n",
        "    if len(word) > 1:\n",
        "        word = re.sub(r'[^a-zA-Z0-9]', '', word)\n",
        "    if word is None or word == \"\" or word == \"nan\":\n",
        "        word = \" \"\n",
        "    return word\n",
        "\n",
        "def read_split_fold(split='dev', fold=\"0\", label_dict=None):\n",
        "    #change the path template as needed.\n",
        "    path = 'Output/folds_{}_{}.tsv'.format(fold, split)\n",
        "    try:\n",
        "        data = pd.read_csv(path, sep='\\t', skip_blank_lines=True,\n",
        "                           encoding='utf-8', engine='python', quoting=csv.QUOTE_NONE,\n",
        "                           names=['Document', 'Sentence-Token', 'Chars', 'Word', 'Tag', 'Empty'], header=None)\n",
        "    except:\n",
        "        print(f\"Cannot read the file {path}\")\n",
        "        if split == \"train\":\n",
        "            sys.exit()\n",
        "        return None, None\n",
        "\n",
        "    time.sleep(5)\n",
        "    data.drop('Empty', inplace=True, axis=1)\n",
        "\n",
        "    #For the reusability purposes, we still extract the label ids from the training data.\n",
        "    data['Tag'] = data.apply(lambda row: to_clean_label(row), axis=1)\n",
        "\n",
        "    print(\"Number of tags: {}\".format(len(data.Tag.unique())))\n",
        "    frequencies = data.Tag.value_counts()\n",
        "    print(frequencies)\n",
        "\n",
        "    if not label_dict:\n",
        "        labels_to_ids = {k: v for v, k in enumerate(data.Tag.unique())}\n",
        "    else:\n",
        "        labels_to_ids = label_dict\n",
        "\n",
        "    ids_to_labels = {v: k for v, k in enumerate(data.Tag.unique())}\n",
        "\n",
        "    data = data.astype({\"Word\": str})\n",
        "\n",
        "    data['Word'] = data.apply(lambda row: replace_punctuation(row), axis=1)\n",
        "    data['Tag'] = data.apply(lambda row: to_label_id(row, labels_to_ids), axis=1)\n",
        "    data['Num'] = data.apply(lambda row: sentence_num(row), axis=1)\n",
        "\n",
        "    # Important point is that we need unique document+Sentence-Token\n",
        "    data = data.astype({\"Num\": int})\n",
        "    data.set_index(['Document', 'Num'])\n",
        "    df = data.groupby(['Document', 'Num'])['Word'].apply(list)\n",
        "    df2 = data.groupby(['Document', 'Num'])['Tag'].apply(list)\n",
        "    mergeddf = pd.merge(df, df2, on=['Document', 'Num'])\n",
        "    mergeddf.rename(columns={'Word': 'sentence', 'Tag': 'word_labels'}, inplace=True)\n",
        "\n",
        "    print(\"Number of unique sentences: {}\".format(len(mergeddf)))\n",
        "\n",
        "    return mergeddf, labels_to_ids, ids_to_labels\n",
        "\n",
        "\n",
        "def tokenize_and_align_labels(examples, tokenizer, label_all_tokens=True):\n",
        "    tokenized_inputs = tokenizer(examples[\"sentence\"], max_length=512, truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"word_labels\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "def cn_hp_space(trial):\n",
        "\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [1e-5, 2e-5, 3e-5, 4e-5, 5e-5]),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8]),\n",
        "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 10, log=True)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaWAboqfFoVN",
        "outputId": "64e7b0bc-070e-4765-c114-0ddde07a3f50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'LABEL_0': 0, 'LABEL_1': 1}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
        "labels_to_ids = config.label2id\n",
        "labels_to_ids\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381,
          "referenced_widgets": [
            "10ef5585f5c9407db3af8614c7aa8603",
            "91b82e52230b4d93acb3deaf9aa4583a",
            "de81c0f3913346a38d3f8e811bc4e702",
            "0cb6560bc8d7408eb6b3875fb4517bae",
            "21aae525701c4b02a4f2b8adfc9d7b21",
            "38711d1cc5c043bab4c86eb67555fbb6",
            "0a29c0583c9846b9913437465014a1d2",
            "e6415927d6e14038b4493ea809783ca3",
            "3c32d2a30e8b4acd87d50af27fd97c6f",
            "2b11638e567a4e88a5103e3f6b98b803",
            "87f64a0dbf68476a94011a2aa93b9fe3"
          ]
        },
        "id": "EQY0atPf7XPE",
        "outputId": "6f8b0be6-d49b-4785-c480-a95f16855015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST RESULTS\n",
            "Number of tags: 10\n",
            "O                  230672\n",
            "B-Smell_Source       1176\n",
            "I-Smell_Source       1088\n",
            "B-Smell_Word          992\n",
            "B-Quality             736\n",
            "I-Quality             488\n",
            "I-Location             32\n",
            "I-Smell_Word           16\n",
            "B-Location              8\n",
            "B-Odour_Carrier         8\n",
            "Name: Tag, dtype: int64\n",
            "Number of unique sentences: 1114\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1114 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10ef5585f5c9407db3af8614c7aa8603"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "import accelerate\n",
        "from transformers import TFBertForMaskedLM\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "hypsearch = False\n",
        "do_train = False\n",
        "do_test = True\n",
        "# best found with hyperparam search\n",
        "learning_rate = 4e-05\n",
        "train_batch_size = 4\n",
        "train_epochs = 7\n",
        "#defaults from Tonelli code\n",
        "#learning_rate = 4e-5\n",
        "#train_batch_size = 8\n",
        "#train_epochs = 10\n",
        "model = \"bert-base-uncased\"\n",
        "\n",
        "#model_checkpoint = model\n",
        "model_checkpoint = \"bert-base-uncased-english-0-hyp/run-9/checkpoint-2511\"\n",
        "fold = '0'\n",
        "language = 'english'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "output_path = \"Output\"\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
        "\n",
        "if language not in ['english', 'german', 'italian', 'slovene', 'dutch', 'french']:\n",
        "  raise Exception(f\"Language error: {language} is not among the project languages.\")\n",
        "\n",
        "if do_train and hypsearch:\n",
        "  raise Exception(f\"Action error: Cannot do hyperparameter search and train in a single run. Please first run\"\n",
        "                  f\"hypsearch and with the parameters obtained as the best, run do_train.\")\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_checkpoint)\n",
        "labels_to_ids = config.label2id\n",
        "ids_to_labels = config.id2label\n",
        "\n",
        "def model_init():\n",
        "  m = AutoModelForTokenClassification.from_pretrained(model_checkpoint, config=config)\n",
        "  m.to(device)\n",
        "  return m\n",
        "\n",
        "if hypsearch or do_train:\n",
        "  trn, labels_to_ids, ids_to_labels = read_split_fold(fold=fold)\n",
        "  train_dataset = Dataset.from_pandas(trn, split=\"train\")\n",
        "  val, _, _ = read_split_fold(fold=fold, split=\"dev\", label_dict=labels_to_ids)\n",
        "  val_dataset = Dataset.from_pandas(val, split=\"validation\")\n",
        "\n",
        "  print(labels_to_ids)\n",
        "  tokenized_train = train_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer), batched=True)\n",
        "  tokenized_val = val_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer), batched=True)\n",
        "  label_list = list(labels_to_ids.values())\n",
        "  config.label2id = labels_to_ids\n",
        "  config.id2label = ids_to_labels\n",
        "  config.num_labels = len(label_list)\n",
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "if hypsearch:\n",
        "  tr_args = TrainingArguments(\n",
        "      f\"{model_name}-{language}-{fold}-hyp\",\n",
        "      evaluation_strategy=\"epoch\",\n",
        "      save_strategy=\"epoch\",\n",
        "      per_device_eval_batch_size=8,\n",
        "      warmup_ratio=0.1,\n",
        "      seed=22,\n",
        "      weight_decay=0.01\n",
        "    )\n",
        "elif do_train:\n",
        "  tr_args = TrainingArguments(\n",
        "      f\"{model_name}-{language}-{fold}\",\n",
        "      evaluation_strategy=\"epoch\",\n",
        "      save_strategy=\"epoch\",\n",
        "      learning_rate=learning_rate,\n",
        "      per_device_train_batch_size=train_batch_size,\n",
        "      per_device_eval_batch_size=8,\n",
        "      num_train_epochs=train_epochs,\n",
        "      warmup_ratio=0.1,\n",
        "      seed=22,\n",
        "      weight_decay=0.01\n",
        "    )\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "  predictions, labels = p\n",
        "  predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "  # Remove ignored index (special tokens)\n",
        "  true_predictions = [\n",
        "      [ids_to_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "      for prediction, label in zip(predictions, labels)\n",
        "  ]\n",
        "  true_labels = [\n",
        "      [ids_to_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "      for prediction, label in zip(predictions, labels)\n",
        "  ]\n",
        "\n",
        "  results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "  return {\n",
        "      \"precision\": results[\"overall_precision\"],\n",
        "      \"recall\": results[\"overall_recall\"],\n",
        "      \"f1\": results[\"overall_f1\"],\n",
        "      \"accuracy\": results[\"overall_accuracy\"],\n",
        "  }\n",
        "\n",
        "\n",
        "if do_train or hypsearch:\n",
        "  trainer = Trainer(\n",
        "      model_init=model_init,\n",
        "      args=tr_args,\n",
        "      train_dataset=tokenized_train,\n",
        "      eval_dataset=tokenized_val,\n",
        "      data_collator=data_collator,\n",
        "      tokenizer=tokenizer,\n",
        "      compute_metrics=compute_metrics\n",
        "  )\n",
        "elif do_test:\n",
        "  #for testing\n",
        "  if path.exists(f\"{model_checkpoint}/{language}-id2label.json\"):\n",
        "    ids_to_labels = json.load(open(f\"{model_checkpoint}/{language}-id2label.json\", \"r\"))\n",
        "    ids_to_labels = {int(k): v for k, v in ids_to_labels.items()}\n",
        "    labels_to_ids = {v: int(k) for k, v in ids_to_labels.items()}\n",
        "    config.label2id = labels_to_ids\n",
        "    config.id2label = ids_to_labels\n",
        "    label_list = list(labels_to_ids.values())\n",
        "    config.num_labels = len(label_list)\n",
        "\n",
        "  m = AutoModelForTokenClassification.from_pretrained(model_checkpoint, config=config)\n",
        "  m.to(device)\n",
        "  trainer = Trainer(m, data_collator=data_collator, tokenizer=tokenizer)\n",
        "\n",
        "if hypsearch:\n",
        "  # hyperparam search with compute_metrics: default maximization is through the sum of all the metrics returned\n",
        "  best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\", hp_space=cn_hp_space)\n",
        "  best_params = best_run.hyperparameters\n",
        "  print(f\"Best run is with the hyperparams:{best_params}. You either have to find the right run and checkpoint \"\n",
        "        f\"from the models saved or retrain with the correct parameters: referring to \"\n",
        "        f\"https://discuss.huggingface.co/t/accessing-model-after-training-with-hyper-parameter-search/20081\")\n",
        "\n",
        "elif do_train:\n",
        "  trainer.train()\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "  def default(self, obj):\n",
        "      if isinstance(obj, np.integer):\n",
        "          return int(obj)\n",
        "      elif isinstance(obj, np.floating):\n",
        "          return float(obj)\n",
        "      elif isinstance(obj, np.ndarray):\n",
        "          return obj.tolist()\n",
        "      return super(NumpyEncoder, self).default(obj)\n",
        "\n",
        "if do_test:\n",
        "  print(\"TEST RESULTS\")\n",
        "  test, _, _ = read_split_fold(split=\"train\", label_dict=labels_to_ids, fold=fold)\n",
        "  test_dataset = Dataset.from_pandas(test, split=\"train\")\n",
        "  tokenized_test = test_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer),\n",
        "                                    batched=True)\n",
        "  encoded_sents = []\n",
        "  for batch in tokenized_test['input_ids']:\n",
        "    encoded_sents.append(tokenizer.convert_ids_to_tokens(batch)[1:-1])\n",
        "\n",
        "  predictions, labels, _ = trainer.predict(tokenized_test)\n",
        "  # print(predictions.shape)\n",
        "  predictions = np.argmax(predictions, axis=2)\n",
        "  # print('Predictions', predictions.shape)\n",
        "    # Convert predictions and labels to human-readable format\n",
        "\n",
        "  # TODO have a datastructure that maps original tokens to subtokens\n",
        "  # this is the hardest part\n",
        "  # record how tokens are matched with the subtoken indices\n",
        "  # gap between len 75 and 82\n",
        "\n",
        "  readable_labels = [\n",
        "      [ids_to_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "      for prediction, label in zip(predictions, labels)\n",
        "  ]\n",
        "\n",
        "    # Convert predictions and labels to human-readable format\n",
        "  readable_predictions = [\n",
        "      [(p, l) for (p, l) in zip(prediction, label) if l != -100]\n",
        "      for prediction, label in zip(predictions, labels)\n",
        "  ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate human-readable output\n",
        "def generate_readable_output(predictions, labels, sentences, encoded_sents):\n",
        "    output_list = []\n",
        "    for sentence, preds, true_labels, sent in zip(sentences, predictions, labels, encoded_sents):\n",
        "        sentence_output = {\"sentence\": sentence, \"subtokens\": sent, \"entities\": []}\n",
        "        for pred, true_label, subtoken in zip(preds, true_labels, sent):\n",
        "            # Check if the label is not a padding token (-100)\n",
        "            if true_label != -100:\n",
        "                # Convert NumPy arrays to Python lists\n",
        "                # Use .item() to get the scalar value from the NumPy array\n",
        "                if true_label != 'O':\n",
        "                  if (pred != true_label):\n",
        "                      if len(pred) != 0:\n",
        "                          entity = {\"word\": subtoken,\n",
        "                                    \"predicted_label\": ids_to_labels[pred[0]],\n",
        "                                    \"true_label\": true_label}\n",
        "                          if entity[\"predicted_label\"] != entity[\"true_label\"]:\n",
        "                              sentence_output[\"entities\"].append(entity)\n",
        "        output_list.append(sentence_output)\n",
        "\n",
        "    # Serialize the output_list to JSON using the custom encoder\n",
        "    json_filename = \"output_train.json\"\n",
        "    with open(json_filename, 'w') as json_file:\n",
        "        json.dump(output_list, json_file, cls=NumpyEncoder)\n",
        "    print(f\"Test results saved to {json_filename}.\")\n",
        "\n",
        "\n",
        "original_sentences = test_dataset[\"sentence\"]\n",
        "# Generate human-readable output\n",
        "output_list = generate_readable_output(readable_predictions, readable_labels, original_sentences, encoded_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVz_7R2X0wyq",
        "outputId": "153e1eae-7320-4e8c-e5b4-7243d64895de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test results saved to output_train.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SDTvACZLYTs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load the JSON file\n",
        "with open('output_train.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Filter out sentences with empty entity lists\n",
        "data = [entry for entry in data if entry['entities']]\n",
        "\n",
        "# Function to format the entities list for a sentence\n",
        "def format_entities(entities):\n",
        "    formatted = \"\"\n",
        "    for entity in entities:\n",
        "        word = ' '.join(entity['word'])\n",
        "        predicted_label = entity['predicted_label']\n",
        "        true_label = entity['true_label']\n",
        "        formatted += f\"\\n\\\"{word}\\\"\\t\\t{predicted_label}\\t\\t{true_label}\"\n",
        "    return formatted\n",
        "\n",
        "# Write formatted sentences to a .txt file\n",
        "with open('formatted_sentences_train.txt', 'w') as outfile:\n",
        "    for entry in data:\n",
        "        sentence = ' '.join(entry['sentence'])\n",
        "        subtokens = ' '.join(entry[\"subtokens\"])\n",
        "        entities = entry['entities']\n",
        "        formatted_entities = format_entities(entities)\n",
        "        outfile.write(f\"\\nsentence: {sentence}\\n\\nentities: [{formatted_entities}\\n]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(original_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "nnMQkzOBwGq8",
        "outputId": "8b374a9e-adad-4d4c-ae64-00ff2b92013f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82\n",
            "82\n",
            "82\n",
            "75\n",
            "['enormous', 'tropical', 'forests', ',', 'little', 'known', 'to', 'man', ',', 'and', 'from', 'which', 'he', 'gathers', 'here', 'and', 'there', 'the', 'treasures', 'for', 'our', 'orchid', '-', 'and', 'greenhouses', ';', 'great', 'island', 'conservatories', 'like', 'Java', 'and', 'Ceylon', 'and', 'Borneo', ',', 'rich', 'in', 'spices', 'and', 'lovely', 'plant', 'life', ':', 'Australian', 'Bush', ',', 'with', 'traces', 'of', 'plant', 'life', 'as', 'if', 'from', 'another', 'world', ',', 'but', 'often', 'most', 'delicate', 'in', 'odour', 'even', 'in', 'the', 'fragments', 'of', 'them', 'we', 'see', 'in', 'our', 'greenhouses']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'true_label' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-0ed9d9b55be9>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'true_label' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10ef5585f5c9407db3af8614c7aa8603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91b82e52230b4d93acb3deaf9aa4583a",
              "IPY_MODEL_de81c0f3913346a38d3f8e811bc4e702",
              "IPY_MODEL_0cb6560bc8d7408eb6b3875fb4517bae"
            ],
            "layout": "IPY_MODEL_21aae525701c4b02a4f2b8adfc9d7b21"
          }
        },
        "91b82e52230b4d93acb3deaf9aa4583a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38711d1cc5c043bab4c86eb67555fbb6",
            "placeholder": "​",
            "style": "IPY_MODEL_0a29c0583c9846b9913437465014a1d2",
            "value": "Map: 100%"
          }
        },
        "de81c0f3913346a38d3f8e811bc4e702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6415927d6e14038b4493ea809783ca3",
            "max": 1114,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c32d2a30e8b4acd87d50af27fd97c6f",
            "value": 1114
          }
        },
        "0cb6560bc8d7408eb6b3875fb4517bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b11638e567a4e88a5103e3f6b98b803",
            "placeholder": "​",
            "style": "IPY_MODEL_87f64a0dbf68476a94011a2aa93b9fe3",
            "value": " 1114/1114 [00:04&lt;00:00, 247.58 examples/s]"
          }
        },
        "21aae525701c4b02a4f2b8adfc9d7b21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38711d1cc5c043bab4c86eb67555fbb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a29c0583c9846b9913437465014a1d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6415927d6e14038b4493ea809783ca3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c32d2a30e8b4acd87d50af27fd97c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b11638e567a4e88a5103e3f6b98b803": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87f64a0dbf68476a94011a2aa93b9fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}